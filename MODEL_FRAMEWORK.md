# QMIX模型框架说明

## 🏗️ 整体架构

### 系统组成

1. **环境层 (Environment Layer)**
   - GridWorldEnv: 多UAV网格世界环境
   - 障碍物管理: ObstacleManager
   - 状态表示: 地图、UAV位置、访问历史

2. **智能体层 (Agent Layer)**
   - Agent Network: 每个UAV的独立Q网络
   - 观察处理: 局部观察 → Q值

3. **混合层 (Mixing Layer)**
   - Mixing Network: 混合各个agent的Q值
   - Hypernetwork: 根据全局状态生成混合权重

4. **训练层 (Training Layer)**
   - Replay Buffer: 经验回放缓冲区
   - TD Loss计算: 时序差分损失
   - 参数更新: RMSprop优化器

---

## 📐 详细架构

### 1. Agent Network (智能体网络)

**结构**:
```
输入: 观察 (obs_dim)
  ↓
FC1: Linear(obs_dim → hidden_dim) + ReLU
  ↓
RNN: GRUCell(hidden_dim → hidden_dim)
  ↓
FC2: Linear(hidden_dim → action_dim)
  ↓
输出: Q值 (action_dim)
```

**特点**:
- 每个UAV有独立的Agent Network
- 使用GRU处理时序信息
- 输出每个动作的Q值

### 2. Mixing Network (混合网络)

**结构**:
```
输入: 
  - Agent Q值: [Q1, Q2, ..., QN]
  - 全局状态: state
  ↓
Hypernetwork:
  - Hyper_W1: state → (N × mixing_hidden_dim)
  - Hyper_B1: state → mixing_hidden_dim
  - Hyper_W2: state → mixing_hidden_dim
  - Hyper_B2: state → 1
  ↓
混合层:
  - Hidden = ReLU(Q × W1 + B1)
  - Q_total = Hidden × W2 + B2
  ↓
输出: 全局Q值 (Q_total)
```

**特点**:
- 使用Hypernetwork根据全局状态生成混合权重
- 保证单调性: ∂Q_total/∂Q_i ≥ 0
- 将局部Q值混合成全局Q值

### 3. 环境 (Environment)

**状态表示**:
- 地图信息: 访问状态、障碍物、UAV位置
- UAV信息: 位置、能量、历史动作
- 观察维度: map_size × map_size × 3 + 1 + num_uavs × 3

**奖励函数**:
- 新单元格奖励: +358.74
- 已访问单元格: -31.14
- 障碍物碰撞: -225.17
- 完成覆盖: +1000.0
- 形状奖励: 未访问距离 + 障碍距离

---

## 🔄 训练流程

### 1. 前向传播

```
环境状态 (s_t)
  ↓
各UAV观察 (o_1, o_2, ..., o_N)
  ↓
Agent Networks → Q值 (Q_1, Q_2, ..., Q_N)
  ↓
Mixing Network → 全局Q值 (Q_total)
  ↓
Epsilon-greedy → 动作 (a_1, a_2, ..., a_N)
  ↓
环境执行 → 新状态 (s_{t+1}), 奖励 (r_t)
```

### 2. 经验收集

```
(s_t, a_t, r_t, s_{t+1}, done)
  ↓
存储到 Replay Buffer
```

### 3. 训练更新

```
从 Replay Buffer 采样 batch
  ↓
计算目标Q值:
  Q_target = r + γ × max_a' Q_target(s_{t+1}, a')
  ↓
计算TD Loss:
  Loss = (Q_total(s_t, a_t) - Q_target)²
  ↓
反向传播更新参数
  ↓
定期更新 Target Networks
```

---

## 📊 框架图描述

### 文字描述框架图

```
┌─────────────────────────────────────────────────────────────┐
│                     环境层 (Environment)                      │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐    │
│  │ 地图状态 │  │ UAV位置  │  │ 障碍物   │  │ 访问历史 │    │
│  └──────────┘  └──────────┘  └──────────┘  └──────────┘    │
│                          ↓                                   │
│                   观察 (o_1, o_2, ..., o_N)                  │
└──────────────────────────┬───────────────────────────────────┘
                           ↓
┌─────────────────────────────────────────────────────────────┐
│                   智能体层 (Agent Layer)                      │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐      │
│  │ Agent Net 1  │  │ Agent Net 2  │  │ Agent Net N  │      │
│  │              │  │              │  │              │      │
│  │ o_1 → Q_1    │  │ o_2 → Q_2    │  │ o_N → Q_N    │      │
│  └──────────────┘  └──────────────┘  └──────────────┘      │
│         ↓                  ↓                  ↓              │
│         └──────────────────┴──────────────────┘             │
│                          ↓                                   │
│              Q值 (Q_1, Q_2, ..., Q_N)                       │
└──────────────────────────┬───────────────────────────────────┘
                           ↓
┌─────────────────────────────────────────────────────────────┐
│                   混合层 (Mixing Layer)                       │
│                    ┌──────────────┐                         │
│                    │ Mixing Net   │                         │
│                    │              │                         │
│  Q值 + 全局状态 →  │ Hypernetwork │ → 全局Q值 (Q_total)     │
│                    └──────────────┘                         │
└──────────────────────────┬───────────────────────────────────┘
                           ↓
┌─────────────────────────────────────────────────────────────┐
│                   动作选择 (Action Selection)                 │
│              Epsilon-greedy: argmax(Q_total)                 │
│                   或随机探索 (概率ε)                          │
└──────────────────────────┬───────────────────────────────────┘
                           ↓
┌─────────────────────────────────────────────────────────────┐
│                   环境执行 (Environment Step)                 │
│              执行动作 → 新状态 + 奖励                         │
└──────────────────────────┬───────────────────────────────────┘
                           ↓
┌─────────────────────────────────────────────────────────────┐
│                   经验回放 (Replay Buffer)                    │
│         存储: (s_t, a_t, r_t, s_{t+1}, done)                │
└──────────────────────────┬───────────────────────────────────┘
                           ↓
┌─────────────────────────────────────────────────────────────┐
│                   训练更新 (Training Update)                  │
│         采样batch → 计算TD Loss → 更新参数                   │
└─────────────────────────────────────────────────────────────┘
```

---

## 🎨 可视化框架图建议

### 论文中的框架图应包含：

1. **整体架构图**
   - 环境 → 智能体 → 混合网络 → 动作选择 → 环境
   - 显示数据流

2. **Agent Network详细图**
   - 输入层 → FC1 → RNN → FC2 → 输出层
   - 标注维度

3. **Mixing Network详细图**
   - Agent Q值输入
   - Hypernetwork结构
   - 混合过程
   - 全局Q值输出

4. **训练流程图**
   - 经验收集
   - 经验回放
   - TD Loss计算
   - 参数更新

---

## 📝 论文中的框架描述

### Methodology章节建议结构

1. **环境设置**
   - GridWorld环境描述
   - 状态表示
   - 动作空间
   - 奖励函数

2. **QMIX算法**
   - Agent Network架构
   - Mixing Network架构
   - 训练流程

3. **实现细节**
   - 网络参数
   - 训练超参数
   - 优化策略

---

*最后更新: 2025-11-15 18:45*

