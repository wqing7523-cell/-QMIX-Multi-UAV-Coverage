# QMIX vs Q-Learningå¯¹æ¯”åˆ†æ

## ğŸ“Š å¯¹æ¯”è¡¨æ•°æ®è§£è¯»

### å¯¹æ¯”è¡¨æ˜¾ç¤ºçš„æ•°æ®

| éšœç¢å¯†åº¦ | UAVs | Q-Learning | QMIX | Difference |
|---------|------|------------|------|------------|
| 0.00 | 1 | 0.963 | 0.739 | -0.224 |
| 0.00 | 2 | 0.989 | 0.959 | -0.030 |
| 0.00 | 3 | 0.998 | 0.995 | -0.003 |
| 0.10 | 1 | 0.875 | 0.622 | -0.253 |
| 0.10 | 2 | 0.902 | 0.882 | -0.020 |
| 0.10 | 3 | 0.906 | 0.900 | -0.006 |

**è§‚å¯Ÿ**ï¼šæ‰€æœ‰Differenceå€¼éƒ½æ˜¯è´Ÿæ•°ï¼Œè¯´æ˜Q-Learningåœ¨å°è§„æ¨¡åœºæ™¯ä¸‹æ€§èƒ½æ›´å¥½ã€‚

---

## â“ è¿™æ˜¯å¦ä¸è®ºæ–‡ç»“è®ºçŸ›ç›¾ï¼Ÿ

### âœ… **ç­”æ¡ˆï¼šä¸çŸ›ç›¾ï¼**

**åŸå› **ï¼š

### 1. **å®éªŒè§„æ¨¡ä¸åŒ**

#### Q-Learningå¯¹æ¯”å®éªŒï¼ˆå°è§„æ¨¡ï¼‰
- **åœ°å›¾å¤§å°**ï¼š8Ã—8ï¼ˆ64ä¸ªå•å…ƒæ ¼ï¼‰
- **UAVæ•°é‡**ï¼š1-3æ¶
- **éšœç¢å¯†åº¦**ï¼š0.0, 0.10
- **åœºæ™¯**ï¼šå°è§„æ¨¡ã€ç®€å•åœºæ™¯

#### QMIXä¸»è¦å®éªŒï¼ˆå¤§è§„æ¨¡ï¼‰
- **åœ°å›¾å¤§å°**ï¼š12Ã—12, 16Ã—16, 24Ã—24ï¼ˆ144-576ä¸ªå•å…ƒæ ¼ï¼‰
- **UAVæ•°é‡**ï¼š4-6æ¶
- **éšœç¢å¯†åº¦**ï¼š0.0, 0.05, 0.10, 0.20
- **åœºæ™¯**ï¼šå¤§è§„æ¨¡ã€å¤æ‚åœºæ™¯

**å…³é”®ç‚¹**ï¼š**QMIXçš„å®éªŒè§„æ¨¡æ˜¯Q-Learningçš„2-9å€ï¼**

---

### 2. **è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®ä¸æ˜¯ç›´æ¥æ€§èƒ½å¯¹æ¯”**

#### è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®

1. **æ–¹æ³•æ‰©å±•**ï¼š
   - ä»Q-Learningï¼ˆå•æ™ºèƒ½ä½“ï¼‰æ‰©å±•åˆ°QMIXï¼ˆå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼‰
   - è¿™æ˜¯æ–¹æ³•å­¦ä¸Šçš„åˆ›æ–°ï¼Œä¸æ˜¯æ€§èƒ½ä¸Šçš„ç›´æ¥å¯¹æ¯”

2. **ç¯å¢ƒæ‰©å±•**ï¼š
   - ä»8Ã—8æ‰©å±•åˆ°24Ã—24ï¼ˆé¢ç§¯å¢åŠ 9å€ï¼‰
   - ä»1-3æ¶UAVæ‰©å±•åˆ°4-6æ¶UAV
   - æ·»åŠ éšœç¢åœºæ™¯ï¼ˆåŸè®ºæ–‡æ²¡æœ‰ï¼‰

3. **å¯æ‰©å±•æ€§éªŒè¯**ï¼š
   - è¯æ˜QMIXèƒ½å¤Ÿå¤„ç†æ›´å¤§è§„æ¨¡ã€æ›´å¤æ‚çš„åœºæ™¯
   - è¿™äº›åœºæ™¯æ˜¯Q-Learningéš¾ä»¥å¤„ç†çš„

---

### 3. **ä¸ºä»€ä¹ˆå°è§„æ¨¡åœºæ™¯ä¸‹Q-Learningæ›´å¥½ï¼Ÿ**

#### å¯èƒ½çš„åŸå› 

1. **Q-Learningçš„ä¼˜åŠ¿**ï¼š
   - åœ¨å°è§„æ¨¡åœºæ™¯ä¸‹ï¼ŒQ-Learningçš„è¡¨æ ¼æ–¹æ³•æ›´ç›´æ¥æœ‰æ•ˆ
   - çŠ¶æ€ç©ºé—´å°ï¼ŒQ-tableå¯ä»¥å®Œæ•´è¦†ç›–
   - è®­ç»ƒæ›´å¿«ï¼Œæ”¶æ•›æ›´ç¨³å®š

2. **QMIXçš„åŠ£åŠ¿**ï¼š
   - å¤šæ™ºèƒ½ä½“åè°ƒéœ€è¦æ›´å¤šè®­ç»ƒ
   - åœ¨å°è§„æ¨¡åœºæ™¯ä¸‹ï¼Œåè°ƒçš„ä¼˜åŠ¿ä¸æ˜æ˜¾
   - ç½‘ç»œå¤æ‚åº¦é«˜ï¼Œéœ€è¦æ›´å¤šæ•°æ®

3. **è§„æ¨¡æ•ˆåº”**ï¼š
   - **å°è§„æ¨¡**ï¼šQ-Learningæ›´ç®€å•ï¼Œæ›´æœ‰æ•ˆ
   - **å¤§è§„æ¨¡**ï¼šQMIXçš„åä½œä¼˜åŠ¿æ‰èƒ½ä½“ç°

---

### 4. **è®ºæ–‡åº”è¯¥å¼ºè°ƒä»€ä¹ˆï¼Ÿ**

#### âœ… åº”è¯¥å¼ºè°ƒçš„

1. **æ–¹æ³•åˆ›æ–°**ï¼š
   - QMIXæ˜¯å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç®—æ³•
   - èƒ½å¤Ÿå¤„ç†å¤šUAVåä½œé—®é¢˜
   - è¿™æ˜¯Q-Learningæ— æ³•ç›´æ¥å¤„ç†çš„

2. **å¯æ‰©å±•æ€§**ï¼š
   - QMIXèƒ½å¤Ÿå¤„ç†24Ã—24çš„å¤§è§„æ¨¡åœºæ™¯
   - Q-Learningåœ¨9Ã—9åœºæ™¯ä¸‹å°±å·²ç»å¾ˆå›°éš¾ï¼ˆåŸè®ºæ–‡åªæœ‰6.7%æˆåŠŸç‡ï¼‰
   - QMIXåœ¨24Ã—24åœºæ™¯ä¸‹ä»èƒ½è¾¾åˆ°0.73-0.80çš„è¦†ç›–ç‡

3. **æ–°åœºæ™¯**ï¼š
   - QMIXèƒ½å¤Ÿå¤„ç†éšœç¢åœºæ™¯
   - åŸè®ºæ–‡æ²¡æœ‰éšœç¢åœºæ™¯
   - è¿™æ˜¯æ–°çš„æŒ‘æˆ˜å’Œè´¡çŒ®

#### âŒ ä¸åº”è¯¥å¼ºè°ƒçš„

1. **ç›´æ¥æ€§èƒ½å¯¹æ¯”**ï¼š
   - ä¸è¦ç›´æ¥è¯´"QMIXä¼˜äºQ-Learning"
   - å› ä¸ºå®éªŒè§„æ¨¡ä¸åŒï¼Œä¸å…¬å¹³

2. **å°è§„æ¨¡åœºæ™¯**ï¼š
   - ä¸è¦è¿‡åº¦å¼ºè°ƒå°è§„æ¨¡åœºæ™¯çš„å¯¹æ¯”ç»“æœ
   - è¿™ä¸æ˜¯è®ºæ–‡çš„é‡ç‚¹

---

## ğŸ“ è®ºæ–‡ä¸­çš„æ­£ç¡®è¡¨è¿°æ–¹å¼

### æ–¹å¼1ï¼šå¼ºè°ƒæ–¹æ³•åˆ›æ–°å’Œå¯æ‰©å±•æ€§

```
While Q-Learning has shown effectiveness in small-scale scenarios 
(8Ã—8 maps, 1-3 UAVs), it faces challenges in larger and more 
complex environments. This paper extends the approach to QMIX, 
a multi-agent reinforcement learning algorithm, and demonstrates 
its capability to handle large-scale scenarios (up to 24Ã—24 maps) 
with multiple UAVs (4-6) and obstacle scenarios. The experimental 
results show that QMIX achieves coverage rates of 0.73-0.80 in 
high obstacle density scenarios on 24Ã—24 maps, demonstrating 
good scalability and robustness.
```

### æ–¹å¼2ï¼šæ‰¿è®¤å°è§„æ¨¡åœºæ™¯çš„å·®å¼‚

```
In small-scale scenarios (8Ã—8 maps, 1-3 UAVs), Q-Learning 
demonstrates competitive performance, which is expected given 
its simplicity and directness for small state spaces. However, 
as the problem scale increases, Q-Learning faces scalability 
challenges. This paper explores QMIX as an alternative approach 
for large-scale multi-UAV path planning, demonstrating its 
effectiveness in scenarios with up to 24Ã—24 maps, 4-6 UAVs, 
and obstacle densities up to 0.20.
```

### æ–¹å¼3ï¼šå¼ºè°ƒäº’è¡¥æ€§

```
Q-Learning and QMIX serve different purposes: Q-Learning is 
effective for small-scale scenarios with simple state spaces, 
while QMIX is designed for large-scale multi-agent scenarios 
requiring coordination. This paper focuses on the latter, 
demonstrating QMIX's capability to handle large-scale 
environments (24Ã—24 maps) with multiple UAVs (4-6) and 
obstacle scenarios, which are beyond the scope of traditional 
Q-Learning approaches.
```

---

## ğŸ¯ å…³é”®ç»“è®º

### 1. **ä¸çŸ›ç›¾çš„åŸå› **

- **å®éªŒè§„æ¨¡ä¸åŒ**ï¼šQ-Learningæ˜¯å°è§„æ¨¡ï¼ˆ8Ã—8ï¼‰ï¼ŒQMIXæ˜¯å¤§è§„æ¨¡ï¼ˆ24Ã—24ï¼‰
- **ç›®æ ‡ä¸åŒ**ï¼šè®ºæ–‡çš„ç›®æ ‡ä¸æ˜¯è¯æ˜QMIXåœ¨å°è§„æ¨¡ä¸‹æ›´å¥½ï¼Œè€Œæ˜¯å±•ç¤ºQMIXåœ¨å¤§è§„æ¨¡åœºæ™¯ä¸‹çš„èƒ½åŠ›
- **è´¡çŒ®ä¸åŒ**ï¼šè®ºæ–‡çš„è´¡çŒ®æ˜¯æ–¹æ³•æ‰©å±•å’Œå¯æ‰©å±•æ€§éªŒè¯ï¼Œä¸æ˜¯ç›´æ¥æ€§èƒ½å¯¹æ¯”

### 2. **è®ºæ–‡åº”è¯¥å¼ºè°ƒçš„**

- âœ… **æ–¹æ³•åˆ›æ–°**ï¼šä»Q-Learningæ‰©å±•åˆ°QMIX
- âœ… **å¯æ‰©å±•æ€§**ï¼šèƒ½å¤Ÿå¤„ç†24Ã—24çš„å¤§è§„æ¨¡åœºæ™¯
- âœ… **æ–°åœºæ™¯**ï¼šéšœç¢åœºæ™¯çš„å¤„ç†
- âœ… **å¤šUAVåä½œ**ï¼š4-6æ¶UAVçš„åè°ƒèƒ½åŠ›

### 3. **å¯¹æ¯”è¡¨çš„ä½¿ç”¨å»ºè®®**

#### åœ¨è®ºæ–‡ä¸­å¯ä»¥è¿™æ ·ä½¿ç”¨ï¼š

**Table X: Performance Comparison in Small-Scale Scenarios**

| Obstacle Density | UAVs | Q-Learning | QMIX | Notes |
|------------------|------|------------|------|-------|
| 0.00 | 1-3 | 0.963-0.998 | 0.739-0.995 | Q-Learning performs better in small-scale scenarios |
| 0.10 | 1-3 | 0.875-0.906 | 0.622-0.900 | Q-Learning's advantage is more pronounced with fewer UAVs |

**è¯´æ˜æ–‡å­—**ï¼š
```
Table X shows the performance comparison between Q-Learning and 
QMIX in small-scale scenarios (8Ã—8 maps, 1-3 UAVs). Q-Learning 
demonstrates better performance in these scenarios, which is 
expected given its simplicity for small state spaces. However, 
as shown in our main experiments, QMIX is designed for and 
excels in large-scale scenarios (12Ã—12 to 24Ã—24 maps) with 
multiple UAVs (4-6) and obstacle scenarios, which are beyond 
the scope of traditional Q-Learning approaches.
```

---

## ğŸ’¡ æœ€ç»ˆå»ºè®®

### 1. **é‡æ–°å®šä½è®ºæ–‡è´¡çŒ®**

ä¸è¦å¼ºè°ƒ"QMIXä¼˜äºQ-Learning"ï¼Œè€Œæ˜¯å¼ºè°ƒï¼š
- **æ–¹æ³•æ‰©å±•**ï¼šä»å•æ™ºèƒ½ä½“åˆ°å¤šæ™ºèƒ½ä½“
- **å¯æ‰©å±•æ€§**ï¼šèƒ½å¤Ÿå¤„ç†æ›´å¤§è§„æ¨¡ã€æ›´å¤æ‚çš„åœºæ™¯
- **æ–°åœºæ™¯**ï¼šéšœç¢åœºæ™¯çš„å¤„ç†èƒ½åŠ›

### 2. **åˆç†ä½¿ç”¨å¯¹æ¯”æ•°æ®**

- å¯ä»¥åœ¨Related Workæˆ–Discussionä¸­æåŠå°è§„æ¨¡åœºæ™¯çš„å¯¹æ¯”
- è¯´æ˜Q-Learningåœ¨å°è§„æ¨¡åœºæ™¯ä¸‹çš„ä¼˜åŠ¿
- å¼ºè°ƒQMIXåœ¨å¤§è§„æ¨¡åœºæ™¯ä¸‹çš„å¿…è¦æ€§

### 3. **è®ºæ–‡ç»“æ„å»ºè®®**

- **Introduction**ï¼šå¼ºè°ƒå¤§è§„æ¨¡å¤šUAVåœºæ™¯çš„æŒ‘æˆ˜
- **Methodology**ï¼šä»‹ç»QMIXçš„æ–¹æ³•ä¼˜åŠ¿
- **Experiments**ï¼šå±•ç¤ºå¤§è§„æ¨¡åœºæ™¯çš„å®éªŒç»“æœ
- **Results**ï¼šåˆ†æQMIXåœ¨å¤§è§„æ¨¡åœºæ™¯ä¸‹çš„æ€§èƒ½
- **Discussion**ï¼šè®¨è®ºå°è§„æ¨¡vså¤§è§„æ¨¡çš„åŒºåˆ«ï¼Œè¯´æ˜QMIXçš„é€‚ç”¨åœºæ™¯

---

## âœ… æ€»ç»“

**è¿™ä¸ªå¯¹æ¯”è¡¨ä¸çŸ›ç›¾ï¼Œå› ä¸º**ï¼š

1. âœ… å®éªŒè§„æ¨¡ä¸åŒï¼ˆ8Ã—8 vs 24Ã—24ï¼‰
2. âœ… è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®ä¸æ˜¯ç›´æ¥æ€§èƒ½å¯¹æ¯”
3. âœ… è®ºæ–‡å¼ºè°ƒæ–¹æ³•åˆ›æ–°å’Œå¯æ‰©å±•æ€§
4. âœ… QMIXçš„ä¼˜åŠ¿åœ¨å¤§è§„æ¨¡åœºæ™¯ä¸­ä½“ç°

**è®ºæ–‡åº”è¯¥å¼ºè°ƒ**ï¼š
- QMIXèƒ½å¤Ÿå¤„ç†Q-Learningéš¾ä»¥å¤„ç†çš„å¤§è§„æ¨¡åœºæ™¯
- è¿™æ˜¯æ–¹æ³•å­¦ä¸Šçš„æ‰©å±•å’Œåˆ›æ–°
- å¯æ‰©å±•æ€§å’Œæ–°åœºæ™¯çš„å¤„ç†èƒ½åŠ›

---

*åˆ†æå®Œæˆæ—¶é—´: 2025-11-15 19:20*

